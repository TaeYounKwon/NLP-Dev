패딩(Padding) 요약

패딩 전처리란?
- 가변적 길이를 가지는 문장을 같은 길이로 맞춰주기 위해 사용됨
- 이 패딩 과정은 길이가 부족한 문장을 지정된 길이에 맞도록 0(zero padding)로 채워주는데
- 앞에서부터 채우는 pre-padding과 뒤에서 부터 채우는 post-padding으로 나뉨
- 나머지는 실습 코드 확인

post-padding vs pre-padding
- post-padding보다는 pre-padding이 성능이 더 좋고 많이 쓰임
-> 왜? NLP 모델(LSTM, GRU)같은 Recurrent Model의 입력 데이터의 순차적인 특성을 모델링함
-> 문장의 단어들이 차례대로 입력되기에, 마지막 단어가 입력으로 들어갈 때는 앞 단어들의 
-> 시퀀스 모델링이 반영된 가장 중요한 상태
-> pre-padding의 경우 앞단에 0이 채워져 가장 중요한 마지막 단어에 0값이 들어갈 일이 없음
-> 반대로, post-padding의 경우 뒷단에 0이 채워지기에 가장 중요한 마지막 값에 0값이 들어가게됨
--> post-padding은 시퀀스 모델링을 수행하여 뒷단의 입력이 중요한 recurrent model 입장에서 
--> 뒤로 갈수록 피처가 희미해지는 long-dependency현상을 심화시킴
--> pre: 80%, post: 50% 정도로 성능차이가 많이남
--> CNN의 경우 스퀀스 모델이 아니기에 pre/post 모두 75%정도로 차이가 거의 나지 않음
