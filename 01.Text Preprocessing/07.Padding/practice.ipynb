{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_token = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A barber is a person ', 'a barber is good person ', 'a barber is huge person ', 'he Knew A Secret ', 'The Secret He Kept is huge secret ', 'Huge secret ', 'His barber kept his word ', 'a barber kept his word ', 'His barber kept his secret ', 'But keeping and keeping such a huge secret to himself was driving the barber crazy ', 'the barber went up a huge mountain ']\n"
     ]
    }
   ],
   "source": [
    "# Cleaning으로 각 문장별 특수문자, 구두점 삭제\n",
    "sentence = []\n",
    "for s in s_token:\n",
    "    tmp = re.sub('[^a-zA-Z0-9]',' ',s)\n",
    "    sentence.append(tmp)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "# Stopwords로 불용어 삭제\n",
    "pp_sentences = []\n",
    "s_words = set(stopwords.words('english'))\n",
    "\n",
    "for s in sentence:\n",
    "    # word tokenization\n",
    "    word = word_tokenize(s)\n",
    "    result = []\n",
    "    for w in word:\n",
    "        w = w.lower()\n",
    "        if w not in s_words:\n",
    "            result.append(w)\n",
    "    pp_sentences.append(result)\n",
    "print(pp_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n",
      "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
     ]
    }
   ],
   "source": [
    "# OOV를 사용하지 않고 모든 단어에 대해 인코딩한 버전\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(pp_sentences)\n",
    "encoded = tokenizer.texts_to_sequences(pp_sentences)\n",
    "print(encoded)\n",
    "\n",
    "# OOV를 사용하여 최상의 5개의 단어만 사용한 버전\n",
    "vocab_size = 5\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')\n",
    "tokenizer.fit_on_texts(pp_sentences)\n",
    "encoded = tokenizer.texts_to_sequences(pp_sentences)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(item) for item in encoded)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 6, 0, 0, 0, 0, 0],\n",
       "       [2, 1, 6, 0, 0, 0, 0],\n",
       "       [2, 4, 6, 0, 0, 0, 0],\n",
       "       [1, 3, 0, 0, 0, 0, 0],\n",
       "       [3, 5, 4, 3, 0, 0, 0],\n",
       "       [4, 3, 0, 0, 0, 0, 0],\n",
       "       [2, 5, 1, 0, 0, 0, 0],\n",
       "       [2, 5, 1, 0, 0, 0, 0],\n",
       "       [2, 5, 3, 0, 0, 0, 0],\n",
       "       [1, 1, 4, 3, 1, 2, 1],\n",
       "       [2, 1, 4, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# while문을 사용하여 최고길이(max_len)만큼 뒤에 0을 추가(post-zero-padding)\n",
    "for s in encoded:\n",
    "    while len(s) <max_len:\n",
    "        s.append(0)\n",
    "\n",
    "padded_np = np.array(encoded)\n",
    "padded_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras를 사용하여 Padding 작업 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
     ]
    }
   ],
   "source": [
    "# OOV를 사용하여 최상의 5개의 단어만 사용한 버전\n",
    "encoded = tokenizer.texts_to_sequences(pp_sentences)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 2, 6],\n",
       "       [0, 0, 0, 0, 2, 1, 6],\n",
       "       [0, 0, 0, 0, 2, 4, 6],\n",
       "       [0, 0, 0, 0, 0, 1, 3],\n",
       "       [0, 0, 0, 3, 5, 4, 3],\n",
       "       [0, 0, 0, 0, 0, 4, 3],\n",
       "       [0, 0, 0, 0, 2, 5, 1],\n",
       "       [0, 0, 0, 0, 2, 5, 1],\n",
       "       [0, 0, 0, 0, 2, 5, 3],\n",
       "       [1, 1, 4, 3, 1, 2, 1],\n",
       "       [0, 0, 0, 2, 1, 4, 1]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keras의 pad_sequences는 default로 pre-zero-padding 기법을 사용\n",
    "padded = pad_sequences(encoded) \n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 6, 0, 0, 0, 0, 0],\n",
       "       [2, 1, 6, 0, 0, 0, 0],\n",
       "       [2, 4, 6, 0, 0, 0, 0],\n",
       "       [1, 3, 0, 0, 0, 0, 0],\n",
       "       [3, 5, 4, 3, 0, 0, 0],\n",
       "       [4, 3, 0, 0, 0, 0, 0],\n",
       "       [2, 5, 1, 0, 0, 0, 0],\n",
       "       [2, 5, 1, 0, 0, 0, 0],\n",
       "       [2, 5, 3, 0, 0, 0, 0],\n",
       "       [1, 1, 4, 3, 1, 2, 1],\n",
       "       [2, 1, 4, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# post-padding을 원하면, padding='post'를 사용\n",
    "padded_post = pad_sequences(encoded, padding='post')\n",
    "padded_post\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# padding='post'를 사용했을 때에는 위의 numpy과정과 같은 결과 출력됨\n",
    "print(padded_np == padded_post)\n",
    "print((padded_np == padded_post).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 2, 6],\n",
       "       [0, 0, 2, 1, 6],\n",
       "       [0, 0, 2, 4, 6],\n",
       "       [0, 0, 0, 1, 3],\n",
       "       [0, 3, 5, 4, 3],\n",
       "       [0, 0, 0, 4, 3],\n",
       "       [0, 0, 2, 5, 1],\n",
       "       [0, 0, 2, 5, 1],\n",
       "       [0, 0, 2, 5, 3],\n",
       "       [4, 3, 1, 2, 1],\n",
       "       [0, 2, 1, 4, 1]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모든 문서를 max_len으로 정의 할 필요는 없음!, maxlen을 커스텀 설정 가능\n",
    "# 단, 기존데이터가 maxlen보다 길었다면 뒤의 데이터가 손실됨.\n",
    "padded = pad_sequences(encoded,padding='pre', maxlen=5)\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 6, 0, 0, 0, 0, 0],\n",
       "       [2, 1, 6, 0, 0, 0, 0],\n",
       "       [2, 4, 6, 0, 0, 0, 0],\n",
       "       [1, 3, 0, 0, 0, 0, 0],\n",
       "       [3, 5, 4, 3, 0, 0, 0],\n",
       "       [4, 3, 0, 0, 0, 0, 0],\n",
       "       [2, 5, 1, 0, 0, 0, 0],\n",
       "       [2, 5, 1, 0, 0, 0, 0],\n",
       "       [2, 5, 3, 0, 0, 0, 0],\n",
       "       [1, 1, 4, 3, 1, 2, 1],\n",
       "       [2, 1, 4, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 만약 뒤가 아닌 앞의 데이터가 삭제되도록 설정하려면\n",
    "padded = pad_sequences(encoded,padding='post',maxlen=max_len, truncating='pre')\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zero-padding이 아닌 임의의 값으로 padding값을 사용하고 싶다면\n",
    "val_last = len(tokenizer.word_index) + 1\n",
    "val_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15, 15, 15, 15, 15,  2,  6],\n",
       "       [15, 15, 15, 15,  2,  1,  6],\n",
       "       [15, 15, 15, 15,  2,  4,  6],\n",
       "       [15, 15, 15, 15, 15,  1,  3],\n",
       "       [15, 15, 15,  3,  5,  4,  3],\n",
       "       [15, 15, 15, 15, 15,  4,  3],\n",
       "       [15, 15, 15, 15,  2,  5,  1],\n",
       "       [15, 15, 15, 15,  2,  5,  1],\n",
       "       [15, 15, 15, 15,  2,  5,  3],\n",
       "       [ 1,  1,  4,  3,  1,  2,  1],\n",
       "       [15, 15, 15,  2,  1,  4,  1]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(encoded, padding='pre', value=val_last)\n",
    "padded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jkdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
