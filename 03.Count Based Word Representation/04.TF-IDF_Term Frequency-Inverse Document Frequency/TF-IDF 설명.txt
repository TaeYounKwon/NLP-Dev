단어 빈도-역 문서 빈도(Term Frequency-Inverse Document Frequency, TF-IDF) 요약

TF-IDF란?
- 단어의 빈도와 역 문서 빈도(문서의 빈도에 특정 식을 취함)를 사용하여
- DTM 내의 각 단어들만다 중요한 정도를 가중치로 주는 방법
- TF-IDF를 사용하면, 기존의 DTM보다 더 많은 정보를 고려하여 문서 비교 가능
- DTM보다 항상 좋은 성능을 보장는 것은 X, 그러나 많은 경우에서 보다 좋은 성능 확인
- 문서의 유사도를 구하는 작업, 검색 시스템에서 검색 결과의 중요도를 정하는 작업
- 문서 내에서 특정 단어의 중요도를 구하는 작업 등에 쓰임

TF-IDF 구현 방법
- 변수: d - 문서, t - 단어, n - 총 문서의 개수

- tf(d,t): 특정 문서 d에서의 특정 단어 t의 등장 횟수
-> DTM의 결과 값

- df(t): 특정 단어 t가 등장한 문서의 수
-> 몇번 등장했는지는 무시하고, 몇개의 문서에서 특정단어 t가 등장했는지에만 관심

- idf(t): df(t)에 반비례하는 수
->IDF는 DF의 역수! 단, log와 분모에 1을 더해주는데...
--> log를 사용해주지 않는다면, 총 문서의 수 n이 커질수록, IDF의 값은 기하급수적으로 커지게 됨
--> log를 씌워주지 않는다면, 희귀 단어들에 엄청난 가중치가 부여됨 
--> 분모에 1더하기는 분모가 0이 되는 것을 방지

- 계산을 하면 로그(총 문서의 수(항상 일정)/(1+각 단어가 등장한 문서의 수))

실습 후에....
- 각 머신 러닝마다 TF-IDF의 식이 조금씩 상이함 <- 몇가지 문제점이 있어서
- 전체의 문서의 수가 N=4, df(t)의 값이 3이라면 log항의 분자와 분모값이 같아지면서 idf(d,t)의 값이 0이됨.
-> 더이상 가중치의 역할을 수행하지 못함
- 해결방법?
-> sklearn에서는 IDF의 로그항의 분자에 1을 더해줌
-> 로그항에 1을 더해주고
-> TF-IDF에 L2정규화라는 방법으로 값을 조정
-> sklearn에서의 TF-IDF의 희귀 값이 조금 더 낮은 것을 확인해 볼 수 있음