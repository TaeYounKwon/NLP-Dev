{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk vs tensorflow.keras\n",
    "nltk: seperating \"'\" into individual word\n",
    "- ex: 'Don', \"'\", 't' \n",
    "\n",
    "tensorflow.keras: make every word lower case, Not seperating \"'\"\n",
    "- ex: 'don't\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark-sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "# 특수문자 앞 뒤로 자름\n",
    "tmp = \"Don't be fooled by the dark-sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"\n",
    "result1 = word_tokenize(tmp, language=\"english\")\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', '-', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "# 특수문자를 하나의 객체로 인식\n",
    "result = WordPunctTokenizer().tokenize(tmp)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "# 특수문자 -, *등을 제거, '<-의 경우 한번에 붙여서 나옴\n",
    "result = text_to_word_sequence(tmp)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 표준 토큰화 예제\n",
    "\n",
    "### Penn Treebank Tokenization\n",
    "- 규칙 1. 하이푼으로 구성된 단어는 하나로 유지한다.\n",
    "    - dark-sounding\n",
    "- 규칙 2. doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리해준다.\n",
    "    - 'does' & \"n't\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표준 토큰화 예제\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark-sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장 토큰화 예제\n",
    "- 문장 토큰화: 어떻게 문장을 자를것인가...?\n",
    "    - nltk는 단순히 마침표(.)로 문장을 끝내지 않음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm actively looking for Ph.D. Students who are looking foward to get into post-doctoral degree.\", 'and you statisfy all the requirements!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"I'm actively looking for Ph.D. Students who are looking foward to get into post-doctoral degree. and you statisfy all the requirements!\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jkdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
