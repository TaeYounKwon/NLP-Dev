선형 회기(Linear Regression) 요약

선형 회기란?
- 운동하는 시간이 길 수록, 몸무게가 주는것처럼
- 어떤 요인의 수치에 따라서 특정 요인의 수치가 영향을 받는 것
- x: 다른 변수의 값을 변하게 하는 변수
- y: 변수 x에 의해서 값이 종속적으로 변하는 변수

단순 선형 회기 분석(Simple Linear Regression Analysis)
- y = wx+b
- 독립 변수 x가 1개인 경우
- 여기서 w를 머신러닝에서는 가중치(weight), 별도로 더해지는 값 b를 편향(bias)라고 함
- 직선의 방정식에서는 w는 직선의 기울기, b는 절편을 의미

다중 선형 회기 분석(Multiple Linear Regression Analysis)
- y = w1x1 + w2x2 + ... wnxn + b
- 어떤 요인의 수치가 2개 이상일 때, 즉 독립 변수 x가 2개 이상인 경우
- 예) 집 값에 영향을 끼치는 것은, 집의 평수 + 집의 위치 + 방의 개수 + 연식 등등 

가설(Hypothesis) 세우기
- 주어진 데이터에서 x와 y의 관계를 w와 b를 이용하여 식을 세우는 일

비용 함수(Cost Function): 평균 제곱 오차(Mean Square Error, MSE)
- 머신러닝은 위의 가설에서 w와 b를 찾기 위해 실제값과 가설로부터 얻어진 예측값의 오차를 계산하는 식을 세움
- 이 식의 값을 최소화하는 w와 b를 찾아냄
- 이때, 실제값과 예측값에 대한 오차에 대한 식을 

- 목적함수(Objective Function) 
-> 함수의 값을 최소화하거나, 최대화하거나 하는 목적을 가진 함수

- 비용함수(Cost Function) or 손실 함수(Loss Function)
-> 함수의 값을 최소화하려는 목적을 가진 함수
- 위 세 식이 엄밀히는 같은 의미는 아니지만, 위 용어를 같은 의미로 혼용해서 사용됨
- 회귀 문제의 경우 주로 평균 제곱 오차(MSE)방법이 많이 쓰임

평균 제곱 오차는 왜 쓰이는가?
- 오차로 실제값과 예측값을 더하게 되면, 그 수가 양수, 음수로 들쭉날쭉하기에 절대적인 크기를 구할 수 없음
- 먼저 실제값과 예측값을 더한 오차를 계산한 후
- 각 오차를 제곱하여 더하는 방법을 사용 (이게 Sqaure)
- 그 후 데이터의 개수인 n으로 나눔(이게 Mean)
- 이걸 평균 제곱 오차라고 함
- 식: 각(실제값+예측값)^2 / n 
- 이제 이 평균 제곱 오차의 값을 최소값으로 만드는 w와 b를 찾아내는 것이 정답인 직선을 찾아내는 일

옵티마이저(Optimizer): 경사하강법(Gradient Descent)
- 위의 w와 b의 값을 찾기 위해 사용되는 알고리즘이 옵티마이저 또는 최적화 알고리즘이라고 부름
- 이 옵티마이저를 통해 적절한 w와 b를 찾아내는 과정을 머신 러닝에서 훈련 또는 학습이라고 부름

경사하강법이란?
- 가장 기본적인 옵티마이저 알고리즘
- 먼저 기울기(w)와 그에 따른 오차값(cost)의 관계를 이해해야함
- w가 지나치게 높거나 낮을때에는 오차가 커짐
- y = wx+b에서 b를 무시하고 w만 가지고 계산을 해보자면 결국
-> w가 높아지고 낮아짐에 따라 cost가 증가하는 2차함수형태의 그래프가 그려지게됨
-> 목표는 여기서 가장 낮은 볼록한 부분의 맨 아랫부분을 찾는 것
-> 여기서 기계는!
--> 임의의 랜덤값 w값을 정한 뒤, 맨 아래의 볼록한 부분을 향해 점차 w의 값을 수정해나감
--> 이를 가능하게 하는 것이 경사 하강법
--> 여기서는 수학 미분을 배울때 가장 처음 배우게 되는 개념인 한 점에서의 순간 변화율
--> 또는 접선에서의 기울기의 개념을 사용하게 되는데
--> 맨 아래의 볼록한 부분으로 갈수록 접선의 기울기가 점차 작아진다는 점
--> 결국 맨 아래의 볼록한 부분에서는 접선의 기울기가 0이됨
- 정리하자면
-> 경사 하강법의 아이디어는 비용함수를 미분하여 현재 w에서의 접선의 기울기를 구하고
-> 접선의 기울기가 낮은 방향으로 w의 값을 변경하고 다시 미분하고 
-> 이 과정을 접선의 기울기가 0인 곳을 향해 w의 값을 변경하는 작업의 반복

경사하강법을 식으로 나타내자면
- w := w - alpha(learning rate)*미분한(cost(w))
- 위의 식은 현재 w에서의 접선의 기울기와 학습률을 곱한 값을 현재 w에서 빼서 새로운 w의 값으로 한다는 것
- 위의 식을 간단히 보면 w:= w - alpha(음수 기울기) or w:= w - alpha(양수 기울기) 가 되는데
-> 음수 기울기라는 말은, 이차함수에서 음의 방향에서 0의 방향으로 나아간다는 뜻
-> 양수 기울기라는 말은, 이차함수에서 양의 방향에서 0의 방향으로 나아간다는 뜻
- 여기서 학습률(alpha)란?
-> 0~1 사이의 값인데, w를 얼마나 큰 폭으로 이동할지를 결정하게 됨.
-> 약간 보폭과도 비슷함
--> 보폭이 넓으면 빠르게 가겠지만, 중요점을 놓치거나 왔다갔다 할 수 있지만
--> 이걸 cost의 값이 발산한다고 함
--> 보폭이 적으면 정확하게 내려가겠지만 시간이 아주 오래 걸릴 수 있음


