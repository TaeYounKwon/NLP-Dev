소프트맥스 회귀(Softmax Regression) 요약

다중 클래스 분류(Multi-class Classification)이란?
- 이진분류가 2개중 하나였다면, 세개 이상의 선택지중 하나를 고르는 문제가 다중 클래스 분류!
- 예시로는 꽃받침의 길이, 넓이, 꽃잎 길이, 넓이를 통해 붓꽃 품종 예측하는 것
- 앞서 배운 시그모이드 함수를 사용하여 붓꽃 품종을 예측하면 정답확률을 각각 얻을 수는 있지만
-> 전체 확률의 합계가 1이 넘어감...
-> 그럼 전체 확률의 합계가 1인 예측값을 얻도록 할 수 있을까?
-> 그리고 그 경우 확률값이 가장 높은 데이터로 붓꽃의 품종을 예측하면 되지 않을까??
-> 소프트맥스 함수의 출현!!!

소프트맥스 함수(Softmax Function)란?
- 식은 복잡해보이지만
- 선택해야하는 선택지의 총 개수를 k라고 할때, k차원의 벡터를 입력받아 각 클래스에 대한 확률을 추정함
- 분류하고자 하는 클래스가 k개일 때, k차원의 벡터를 입력받아 모든 벡터 원소의 값을 0과 1 사이의 값으로 값을 변경
- 다시 k차원의 벡터를 반환
- 그 후 실제값(예를 들면 setosa의 원 핫 벡터[0,1,0]) 과 softmax로 나온 예측값을 비교함
- 두 벡터의 오차를 계산하기 위해 비용 함수로 크로스 엔트로피 함수 사용, 
- 앞선 배운 선형 회귀나 로지스틱 회귀와 마찬가지로 오차로부터 가중치 업데이트

- 백터로 보자면
- X*W+B = 예측값
-> X: 입력 벡터(3x4) - 3개의 클라스, 4개의 특성
-> W: 가중치 백터(4X1)
-> B: 편향(3X1)
-> 예측값(3X1) 

왜 정수인코딩이 아닌 원-핫 백터를 사용하는가?
- 물론 원-핫 백터를 사용하지 않아도 다중 클래스 분류 문제를 풀 수 있음
- 그러나 대부분의 다중 클래스 분류 문제가 각 클래스 간의 관계가 균등함으로 원-핫 벡터가 잘 쓰임

- 다수의 클래스 분류 문제에서는 각 클래스의 개수만큼 숫자 레이블이 필요
- 이때, 직관적으로는 클래스 전체에 정수 인코딩을 사용할 수 있음(BoW처럼)
- 근데 일반적인 다중 클래스 분류 문제에서는 원-핫 인코딩 >>>> 정수 인코딩임
- 일반적으로 Loss Function으로 MSE를 쓴다면 

-> 정수인코딩은 데이터에 순서가 있는 경우
-> 예) 10대, 20대, 30대, 40대 등 데이터가 관련되어져 있고 순서가 있으면 그 관계를 계산 해 볼 수 있지만
-> 대부분의 경우 순서가 없는 데이터가 들어오기에...
-> 예) 호박, 사과, 딸기, 바나나, 수박
-> 정수 인코딩 사용 시 결과적으로 호박이 사과와 더 비슷하다고 나옴

-> 원-핫 인코딩의 경우 MSE의 결과가 다 비슷하게 나옴으로
-> 결과적으로 원-핫 벡터는 각 클래스의 표현 방법이 무작위성을 가짐

크로스엔트로피 함수(이진분류 -> 소프트맥스회귀)
- 같은 식이 어떻게 다르게 쓰이는지는
- 링크확인!
- https://wikidocs.net/35476
