{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Dictionary 자료형을 이용한 정수 인코딩\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
     ]
    }
   ],
   "source": [
    "# sent_tokenize를 통한 문장 토큰화\n",
    "s_token = sent_tokenize(text)\n",
    "print(s_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A barber is a person ', 'a barber is good person ', 'a barber is huge person ', 'he Knew A Secret ', 'The Secret He Kept is huge secret ', 'Huge secret ', 'His barber kept his word ', 'a barber kept his word ', 'His barber kept his secret ', 'But keeping and keeping such a huge secret to himself was driving the barber crazy ', 'the barber went up a huge mountain ']\n"
     ]
    }
   ],
   "source": [
    "# Cleaning으로 각 문장별 특수문자, 구두점 삭제\n",
    "sentence = []\n",
    "for s in s_token:\n",
    "    tmp = re.sub('[^a-zA-Z0-9]',' ',s)\n",
    "    sentence.append(tmp)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "# Stopwords로 불용어 삭제\n",
    "vocab = {}\n",
    "pp_sentences = []\n",
    "s_words = set(stopwords.words('english'))\n",
    "\n",
    "for s in sentence:\n",
    "    # word tokenization\n",
    "    word = word_tokenize(s)\n",
    "    result = []\n",
    "    for w in word:\n",
    "        w = w.lower()\n",
    "        if w not in s_words:\n",
    "            result.append(w)\n",
    "            if w not in vocab:\n",
    "                vocab[w] = 0\n",
    "            vocab[w]+=1\n",
    "    pp_sentences.append(result)\n",
    "print(pp_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원래는 여기서 어간 추출과 표제어 추출 또한 진행해줘야겠지만\n",
    "# 결과가 덜 정확하게 나옴으로 통과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n"
     ]
    }
   ],
   "source": [
    "# 빈도수 확인\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 빈도수를 기준으로 sorted\n",
    "vocab_sorted = sorted(vocab.items(), key=lambda x:x[1], reverse=True)\n",
    "print(vocab_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
     ]
    }
   ],
   "source": [
    "# 빈도수가 많은 순으로 다시 index를 지정\n",
    "# word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab_sorted)}\n",
    "w_index = {}\n",
    "i = 0\n",
    "for w, f in vocab_sorted:\n",
    "    if f>1: # 빈도수가 적은 단어는 제외\n",
    "        i+=1\n",
    "        w_index[w] = i\n",
    "\n",
    "print(w_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "# 빈도수가 가장 높은 5개의 데이터만 사용\n",
    "vocab_size = 5\n",
    "w_frequency = [word for word, index in w_index.items() if index >=vocab_size+1]\n",
    "for w in w_frequency:\n",
    "    del w_index[w]\n",
    "print(w_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'OOV': 6}\n"
     ]
    }
   ],
   "source": [
    "# w_index를 사용하여 문장을 정수화로 수정\n",
    "# 이 때, w_index에 없는 단어를 위해 OOV(Out-Of-Vocab)를 추가\n",
    "w_index['OOV']= len(w_index)+1\n",
    "print(w_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n"
     ]
    }
   ],
   "source": [
    "# 이제 pp_sentence 내의 모든 단어를 정수화로 인코딩\n",
    "e_sentences = []\n",
    "for sentence in pp_sentences:\n",
    "    e_sentence = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            e_sentence.append(w_index[word])\n",
    "        except KeyError:\n",
    "            e_sentence.append(w_index['OOV'])\n",
    "    e_sentences.append(e_sentence)\n",
    "print(e_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counter을 사용하여 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(pp_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
     ]
    }
   ],
   "source": [
    "# words = np.hstack(preprocessed_sentences)으로도 수행 가능.\n",
    "all_words_list = sum(pp_sentences, [])\n",
    "print(all_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
     ]
    }
   ],
   "source": [
    "# 파이썬의 Counter 모듈을 이용하여 단어의 빈도수 카운트\n",
    "vocab = Counter(all_words_list)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, frequency) in vocab :\n",
    "    i = i + 1\n",
    "    word_to_index[word] = i\n",
    "\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK의 FreqDist를 사용하여 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "print(pp_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "vocab = FreqDist(np.hstack(pp_sentences))\n",
    "print(vocab[\"barber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "# enumerate를 사용하여 더 짧은 코드로 인코딩\n",
    "word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras의 텍스트 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Jack\\miniconda3\\envs\\jkdev\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(pp_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
     ]
    }
   ],
   "source": [
    "# word_index를 통해 빈도수가 높은 단어 확인\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
     ]
    }
   ],
   "source": [
    "# word_counts로 각 단어가 실제로 얼마나 나왔는지 확인\n",
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.texts_to_sequences(pp_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 1) # 상위 5개 단어만 사용\n",
    "tokenizer.fit_on_texts(pp_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n",
      "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
     ]
    }
   ],
   "source": [
    "# 위의 vocab_size를 적용해도 word_index와 word_counts에는 적용되지 않음\n",
    "print(tokenizer.word_index)\n",
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
     ]
    }
   ],
   "source": [
    "# 하지만 texts_to_sequences를 사용할 때 적용됨!\n",
    "print(tokenizer.texts_to_sequences(pp_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word', 'keeping', 'good', 'knew', 'driving', 'crazy', 'went', 'mountain']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_index와 word_counts에도 vocab_size를 적용하고 싶다면\n",
    "vocab_size = 5\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 1) # 상위 5개 단어만 사용\n",
    "tokenizer.fit_on_texts(pp_sentences)\n",
    "w_frequency = [word for word, index in tokenizer.word_index.items() if index>=vocab_size+1]\n",
    "w_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n",
      "OrderedDict([('barber', 8), ('person', 3), ('huge', 5), ('secret', 6), ('kept', 4)])\n"
     ]
    }
   ],
   "source": [
    "for word in w_frequency:\n",
    "    del tokenizer.word_index[word]\n",
    "    del tokenizer.word_counts[word]\n",
    "\n",
    "print(tokenizer.word_index)\n",
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OOV': 1, 'barber': 2, 'secret': 3, 'huge': 4, 'kept': 5, 'person': 6, 'word': 7, 'keeping': 8, 'good': 9, 'knew': 10, 'driving': 11, 'crazy': 12, 'went': 13, 'mountain': 14}\n"
     ]
    }
   ],
   "source": [
    "# 만약 위의 방법 + vocab_size를 초과하는 단어를 OOV로 설정하고 싶다면\n",
    "# 숫자 0과 OOV를 고려해서 단어 집합의 크기는 +2\n",
    "vocab_size = 5\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')\n",
    "tokenizer.fit_on_texts(pp_sentences)\n",
    "print(tokenizer.word_index) # oov_token을 사용하기로 했다면, OOV의 index는 1이됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.texts_to_sequences(pp_sentences))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jkdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
