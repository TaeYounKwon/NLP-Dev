정제(Cleaning) 요약

정제 목적
- 갖고 있는 말뭉치 데이터에서 노이즈를 제거
- 토큰화 작업에 방해가 되는 부분을 배제하기 위해 토큰화 작업 전에 진행
- 토큰화 작업 이후에도 여전히 남아있는 노이즈 제거를 위해 지속적으로 진행
- 완벽한 정제는 어려움 -> 합의점을 찾을 때 까지 진행

여러 정제 방법 - 대소문자 변환
- a Ferrari car의 경우 소문자 변환을 통해 ferrari만 쳐도 관련 차 데이터 내용을 찾을 수 있음
- 주의할 점은, 사람의 이름, 회사 이름, 혹은 나라나 축약어 등(US와 us처럼)
- 대문자를 유지해야 하는 경우를 고려해야함
- 더 많은 변수를 사용해서 언제 소문자 변환을 사용할 지 머신 러닝 시퀀스 모델로 더 정확히 진행 시킬 수 있으나...
-> 예외 사항을 크게 고려하지 않고, 모든 코퍼스를 소문자로 바꾸는 것이 종종 더 실용적인 해결책이 됨.
--> 이 이유는, 데이터 자체가 대소문자를 고려하지 않고 소문자만을 사용해서 글을 적는 사람들한테도 많이 얻어지기 때문에

여러 정제 방법 - 불필요한 단어의 제거
- 노이즈 데이터란? -> 아무 뜻이 없는 문자(특수문자 등) + 분석하고자 하는 목적에 맞지 않는 불필요 단어들

정제 방법 3가지 
- 불용어 제거
-> 추후에 더 설명!!

- 사용 빈도가 적은 단어 제거
-> 전반적으로 어떤 메일이나 글을 분석할 때에 거의 쓰이지 않는 단어

- 길이가 짧은 단어들을 제거
-> 영어권 언어에서는 유용함!
--> 예) a, i, it, at, to, in, by 등등 짧은 단어들이 대부분의 단어 뜻을 가지고 있지 않기 때문에
--> 영어권 나라에서는 길이가 짧은 단어들은 대부분 불용어에 해당됨
--> 이유는 영어 단어의 평균 길이는 6~7
-> 한국어는?
--> 단어가 짧다고 해서 제거하면 안됨!! 한국 단어의 평균 길이는 2~3
--> 한국에는 문자 하나하나가 한자의 뜻을 가지고 있기 때문에
-> 방법으로는 re 라이브러리에서 정규 표현식을 이용하여 삭제

- 정규 표현식 (Regular Expression)을 사용하여 노이즈 데이터 제거
-> 예를 들면 데이터를 크롤링해서 가져왔다면 HTML 태그가 여러군데 있을 수 있음
-> 뉴스 기사라면 시간 데이터 또한 크롤링 되어 얻어졌을 수 도 있고
--> 정규 표현식을 사용해서 계속해서 등장하는 글자들을 규칙에 기반하여 한번에 제거

