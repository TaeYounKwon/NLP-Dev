딥 러닝의 학습 방법 요약

1. 손실함수
- 손실함수는 실제값과 예측값의 차이를 수치화해주는 함수
- 두 값의 차이, 즉 오차가 클 수록 손실 함수의 값은 크고, 오차가 작을 수록 손실 함수의 값이 작아짐
- 회귀에서는 MSE, 분류에서는 크로스 엔트로피를 주로 사용
- 손실함수의 값을 최소화하는 두개의 매개변수인 w(가중치), b(편향)의 값을 찾는 것이 딥 러닝 학습과정
- 손실함수 선정이 매우 중요!
- 아래부터는 앞서 배웠던 손실함수 요약
- 모든 사용 방법은 코드 확인!

MSE(Mean Sqaure Error, MSE)
- 평균제곱 오차 
- 연속형 변수를 예측할 때 사용

Cross-Entropy
- 대부분의 분류 문제에서 사용됨
- 이진분류시 - Binary Cross-Entropy
- 다중클래스 분류시 - Categorical Cross-Entropy
- 원 핫 인코딩 생략, 정수값을 가진 레이블에 다중클래스 분류시 - Sparse-Categorical Cross-Entropy
- 나머지 다양한 손실함수들은 아래 사이트에서 확인
- https://www.tensorflow.org/api_docs/python/tf/keras/losses

2. 배치크기(Batch Size)에 따른 경사 하강법
- 손실 함수의 값을 줄여나가면서 학습하는 방법은 어떤 옵티마이저를 사용하느냐에 따라 달라짐
- 배치(Batch)라는 개념에 대한 이해가 필요
- 배치는 가중치 등의 매개 변수의 값을 조정하기위해 사용하는 데이터의 양
- 전체 다 사용가능, 정해준 양의 데이터만 가지고도 매개 변수 값 조절 가능!

배치 경사 하강법(Batch Gradient Descent)
- 가장 기본적인 경사 하강법
- 옵티마이저 중 하나로 오차(Loss)를 구할 때 전체 데이터를 고려
- 배치 경사 하강법은 한번의 에포크에 모든 매개변수 업데이트를 단 한번 수행함.
- 전체 데이터를 고려해서 학습하므로, 한번의 매개 변수 업데이트에 시간이 오래 걸리며, 메모리가 많이 요구됨

배치 크기가 1인 확률적 경사 하강법(Stochastic Gradient Descent, SGD)
- 위의 방법은 전체 데이터에 대해서 계산하다보니 시간이 너무 오래걸림
- 매개 변수 값을 조정 시 전체 데이터가 아닌 랜덤으로 선택한 하나의 데이터에 대해서만 계산하는 방법
- 더 빠르게 계산 가능
- 경사 하강법보다 정확도가 낮을수도 있지만, 하나의 데이터에 대해서만 메모리에 저장하면 됨으로
- 자원이 적은 컴퓨터에서도 쉽게 사용 가능

미니 배치 경사 하강법(Mini-Batch Gradient Descent)
- 전체도, 1도 아닌 배치 크기를 지정하여 해당 데이터 개수만큼에 대해서 계산하여 매개 변수의 값을 조정하는 방법
- 전체 배치 경사 보다 빠르고, SGD보다 안정적이라는 장점
- 배치크기는 일반적으로 2의 n제곱에 해당하는 숫자로 선택하는 것이 보편적
- model.fit()에서 배치 크기를 별도로 지정해주지 않았을 경우 32로 설정됨

3. 옵티마이저(Optimizer)
모멘텀이란?
- 관성이라는 물리학의 법칙을 응용한 방법
- 모멘텀 경사 하강법에 관성을 더해줌
- 경사하강법에서 계산된 접선의 기울기에 한 시점 전의 접선의 기울기값을 이정한 비율만큼 반영
- 로컬 미니멈을 관성의 힘으로 넘어갈 수 있음
- 하지만...
-> 로컬미니멈 다음에 로컬미니멈이면 그때는 못넘을 수도...

아다그라드(Adagrad)란?
- 매개변수들은 각자 의미하는 바가 다름
- 그래서 모든 매개변수에 동일한 학습률(learning rate)을 적용하는 것은 비효율적
- 아다그라드는 각 매개변수에 서로 다른 학습률을 적용시킴
-> 변화가 많은 매개변수는 학습률이 작게
-> 변화가 적은 매개변수는 학습률을 높게 설정시킴

RMSprop란?
- 아다그라드는 학습을 계속 진행한 경우에는, 나중에 가서 학습률이 지나치게 떨어진다는 단점이 있는데
- 이를 다른 수식으로 대체하여 단점을 개선한 것

아담(adam)이란?
- RMSprop와 모멘텀 두가지를 합친 방법, 방향과 학습률 두 가지를 모두 잡기 위한 방법

4. 에포크(Epoch), 배치사이즈(Batch Size), 이터레이션(Iteration) 혹은 스텝(Step)
에포크란?
- 인공 신경망에서 전체 데이터에 대해서 순전파와 역전파가 끝난 상태
- 하나의 기출고사를 다 풀고, 정답지로 채점해서 기출고사에 대한 공부를 한번 끝난 상태
- 에포크가 50이라면, 전체 데이터 단위로 50번 학습함
- 기출고사를 50번 푼 셈
- 너무많거나 적을때 과적, 과소적합이 발생 할 수 있음

배치 크기란?
- 몇개의 데이터 단위로 매개변수를 업데이트하는지 말하는 것
- 문제를 몇개 풀고 정답지를 확인 할 것인지
- 사람이 20문제씩 문제를 풀고 답안지를 확인한다면, batch size=20
- 주의할 점! 배치 크기 != 배치의 수!!!
- 전체 데이터가 2000이고, 배치 크기가 200이면, 배치의 수는 10
- 이때 배치의 수를 이터레이션이라고 함

이터레이션 또는 스텝이란?
- 어포크를 끝내기 위해서 필요한 배치의 수
- 한번의 에포크 내에서 이루어지는 매개변수의 업데이트 횟수이기도 함
- 전체 데이터가 2,000일 때 배치 크기를 200으로 한다면 이터레이션의 수는 총 10
- 한번의 에포크 당 매개변수 업데이트가 10번 이루어진다는 것

- 배치 크기가 1인 SGD를 가지고 설명하면 배치 크기가 1이므로
- 모든 이터레이션마다 하나의 데이터를 선택해서 경사 하강법을 수행
- 이터레이션은 스텝(Step)이라고 부르기도 함!


