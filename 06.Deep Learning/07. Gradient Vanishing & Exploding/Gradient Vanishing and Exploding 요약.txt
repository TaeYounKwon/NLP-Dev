기울기 소실과 폭주 요약

기울기 소실이란?
- 역전파 과정에서 입력층으로 갈 수록 기울기(Gradient)가 점차적으로 작아지는 현상
- 입력층에 가까운 층들에서 가중치들이 업데이트가 제대로 되지 않아 최적의 모델을 찾을 수 없게 되는 것

폭주란?
- 기울기 소실과는 반대의 경우로 기울기가 점차 커지더니 가중치들이 비정상적으로 큰 값이 되는 것
- 순환 신경망(RNN)에서 쉽게 발생됨

기울기 소실/폭주를 막는 방법들
1. 시그모이드 대신 ReLU나 ReLU의 변형 함수 사용
- 은닉츠에서는 시그모이드 함수 X -> 대신 ReLU나 ReLU의 변형 함수 사용!
- Leaky ReLU를 사용하면 모든 입력값에 대해서 기울기가 0에 수렴하지 않아 죽은 ReLU문제 해결 가능

2. 그래디언트 클리핑(Gradient Clipping)
- 기울기 폭주를 막기 위해 사용됨
- 기울기 값을 임계값이 넘지 않도록 자르는 것을 의미
- 임계치만큼 크기를 감소
- RNN에서 많이 쓰임
- RNN은 역전파 과정에서 시점을 역행하면서 기울기를 구함
-> 이때 기울기가 너무 커질 수 있기에 기울기 값을 자르게 됨
-> 아래 코드와 같이 사용됨
'''
from tensorflow.keras import optimizers
Adam = optimizers.Adam(lr=0.0001, clipnorm=1.)
'''

3. 가중치 초기화 
- 가중치가 초기에 어떤 값을 가졌냐에 따라서 모델의 훈련 결과가 달라지기도 함
- 가중치 초기화만 적절히 해줘도 기울기 소실 문제 어느정도 해결 가능!

3.1. 세이비어 초기화(Xavier Initialization)
- 여러층의 기울기 분산 사이에 균형을 맞춰 특정 층이 너무 주목을 받거나 다른 칭이 뒤쳐지는 것을 막음
- 시그모이드 함수나 하이퍼볼릭 탄젠트 함수와 같은 S자 형태인 활성화 함수와 사용할 경우 좋은 성능
- ReLU와는 성능이 좋지 않음...

3.2. He 초기화
- 세이비어 초기화와 유사하게 정규 분포와 균등 분포 두 가지 경우로 나뉨
- 단, 세이비어와 다르게 다음 층의 뉴런의 수를 반영하지 X
- 세이비어보다 조금더 보편적으로 사용됨

4. 배치 정규화(Batch Normalization)
- 인공 신경망의 각 층에 들어가는 입력을 평균과 분산으로 정규화하여 학습을 효율적으로 만듬

4.1. 내부 공변량 변화(Internal Covariate Shift)
- 학습 과정에서 층 별로 입력 데이터 분포가 달라지는 현상을 말함
- 공변량 변화는 훈련 데이터의 분포와 테스트 데이터의 분포가 다른 경우
- 내부 공변량 변화는 신경망 층 사이에서 발생하는 입력 데이터의 분포 변화를 의미

4.2. 배치 정규화
- 한번에 들어오는 배치 단위로 정규화하는 것
- 입력에 대해 평균을 0으로 만들고 정규화 진행
- 정규화 된 데이터에 대해서는 스케일과 시프트를 수행
- 시그모이드나 하이퍼볼릭탄젠트를 사용해도 기울기 소실 문제가 크게 개선됨
- 가중치 초기화에 훨씬 덜 민감함
- 훨씬 큰 학습률을 사용할 수 있어 학습 속도 개선
- 미니 배치마다 평균과 표준편사를 계산하여 사용
-> 훈련 데이터에 일종의 잡음 주입의 부수 효과로 과적합을 방지
-> 마치 드롭아웃과 비슷한 효과, 물론 드롭아웃과 함께 사용하는 것이 더 좋음
- 모델을 복잡하게 하며, 추가 계산을 하는 것 -> 테스트 데이터에 대한 예측 시에 실행시간 증가됨..
-> 서비스 속도를 고려하는 관점에서는 배치 정규화가 필요한지 고민이 요구됨

4.3. 배치 정규화의 한계
4.3.1 미니 배치 크기에 의존적
- 너무 작은 배치 크기에서는 잘 작동 X
- 단적으로 배치크기가 1일때 분산은 0
- 작은 미니 배치에서는 배치 정규화의 효과가 극단적으로 작용 -> 훈련에 악영향

4.3.2. RNN에서 적용이 힘듬
- RNN은 각 시점(time step)마다 다른 통계치를 가짐
- Batch 정규화 보단 층 정규화(layer normalization) 방법사용됨
