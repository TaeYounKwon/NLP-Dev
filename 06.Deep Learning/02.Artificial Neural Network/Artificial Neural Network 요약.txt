인공 신경망(Artificial Neural Network)요약

피드 포워드 신경망(Feed-Forward Neural Network, FFNN)란?
- 다층 퍼셉트론(MLP)와 같이 오직 입력층에서 출력층 방향으로 연산이 전개되는 신경망
- 한 방향(주로 왼쪽에서 오른쪽)으로 쭉 나아가는 신경망
- FFNN이 아닌것은 RNN
-> 은닉층의 출력값을 출력층으로도 값을 보내지만
-> 동시에 은닉층의 출력값이 다시 은닉층의 입력으로 사용됨

전결합층(Fully-Connected Layer, FC, Dense Layer)
- 어떤 층의 모든 뉴런이 이전 층의 모든 뉴런과 연결돼 있는 층을 전결합층 혹은 완전연결층이라고 함
- 다층 퍼셉트론의 은닉층과 출력층에 있는 모든 뉴런은 바로 이전층의 모든 뉴런과 연결되어있음
- 다층 퍼셉트론의 모든 은닉층과 출력층은 전결합층
- 밀집층(Dense Layer)라고도 함, 케라스에서는 밀집층을 구현할 때 Dense()를 사용

활성화 함수(Activation Function)란?
- 먼저 봤던 일정 임계값(threshold)를 넘으면 0과 1을 배출하는 계단함수(Step Function)이 있었고,
- 은닉층과 출력층의 뉴런에서 출력값을 경정하는 함수를 활성화 함수라고 함

활성화 함수의 특징
- 비선형 함수(Nonlinear function)
-> 직선 1개로는 그릴 수 없는 함수를 뜻함
-> 선형함수란 출력이 입력의 상수배만큼 변하는 함수
->> 대표적으로는 y=wx+b
-> 선형함수를 쓰게 되면, 은닉층을 여러번 추가하더라도 1회 추가한 것과 차이가 없음
--> 활성화 함수가 y=wx라면
--> 결국 y=w(w(wx))) => y=w^3x, w^3=k, 결국 y=kx로
--> 선형함수로 w를 3번 쌓았지만, 결국 k를 1번 쌓은 것과 같다...

- 활성화 함수 함수를 사용하지 않는 층을 비선형층들과 함게 쓰는 경우도 있음
-> 그 경우, 학습 가능한 가중치가 새로 생김
-> 선형함수를 사용한 층을 선형층(linear layer)이나 투사층(projection layer)라고도 함
- 임베딩 층(embedding layer)도 선형층


인공 신경망의 학습과정을 정리하자면
-> 입력에 대해서 순전파(foward propagation) 연산을 하고,
-> 순전파 연산을 통해 나온 예측값과 신제값의 오차를 손실 함수를 통해 계산
-> 이 손실을 미분을 통해서 기울기(gradient)를 구하고
-> 출력층에서 입력층 방향으로 가중치와 편향을 업데이트 하는 과정인 역전파를 수행
--> 역전파란 인공시경망에서 출력층에서 입력층 방향으로 가중치와 편향을 업데이트하는 과정

시그모이드함수의 기울기 소실
- 시그모이드 함수의 0.0과 1.0에 다다르는 값들를 미분하면 0에 가까운 아주 작은 값들임
- 그에 반면 가운데 완만한 선형은 최대 미분값이 0.25
- 즉 시그모이드 함수를 미분한 모든 값은 0~0.25 사이!
- 시그모이드 함수를 활성화 함수로 하여 인공 신경망에 층을 쌓는 다면
-> 가중치와 편향을 업데이트 하는 과정 중 역전파 과정에서 0에 가까운 값이 누적해서 곱해짐
-> 앞단에는 기울기(미분값)가 잘 전달되지 않음
- 다시 말해 w가 업데이트 되지 않음
- 결국 시그모이드 함수의 은닉층에서의 사용은 지양됨
- 주로 출력층에서 사용

그렇다면 답은 하이퍼볼릭탄젠트함수?
- 이 함수 또한 -1과 1와 가까운 값을 출력할 때 시그모이드와같이 기울기 소실이 일어남
- 그러나 하이퍼볼릭탄젠트함수는 시그모이드와는 다르게 0을 중심으로
- 미분의 최대값이 1로 시그모이드 함수의 최대값이 0.25보다는 큼
- 그럼으로 은닉층에서 시그모이드 함수보다는 선호됨

그럼 어떤걸 은닉층에서 사용해야하는가? 답은 ReLU!
- ReLu함수란?
- y = max(0,x)
- 음수를 입력하면 0의 값을 출력, 양수를 입력하면 입력값이 그대로 반환되는 것이 특징인 함수
- 출력값이 특정 양수값에 수렴하지 안음
- 0 이상의 경우에는 미분값이 항상 1
- 깂은 신경망의 은닉층에서 시그모이드 함수보다 훨씬 더 잘 작동
- 시그모이드와 하이퍼볼릭탄젠트처럼 어떤 연산이 필요하지 않은 단순 임계값이므로 연산 속도또한 빠름
- 문제점이 아에 없나요??
-> 입력값이 음수면 기울기, 즉 미분값이 0
-> 이러한 뉴런은 다시 회생하는 것이 매우 어려우며,
-> 죽은 렐루(dying ReLU)라고도 함

ReLU를 보안한 리키 렐루(Leaky ReLU)
- Leaky ReLU의 경우 입력값이 음수일 경우에 0이 아닌 0.001과 같은 매우 작은 수를 반환하도록 만듬
- 여기서 Leaky는 물이 항아리에서 조금씩 "새는"이란 뜻이며, 일반적으로는 0.01의 값을 가짐
- 입력값이 음수라도 기울기가 0이 되지 않아 ReLU가 죽지 않음

소프트맥스 함수
- 은닉층에서는 ReLU나 ReLU의 변형 함수를 사용하는 것이 일반적
- 소프트맥스 함수는 시그모이드 함수처럼 출력층에서 주로 사용
- 시그모이드는 이진분류
- 소프트맥스는 다중 클래스 분류에 주로 사용됨
- 