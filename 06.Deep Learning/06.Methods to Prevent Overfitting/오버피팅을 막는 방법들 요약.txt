과적합을 막는 방법들 요약

과적합을 막는 이유?
- 훈련데이터에 대한 정확도는 증가하지만, 새로운 데이터(검증, 테스트)에 대해서는 제대로 작동하지 않음

과적합을 막는 방법
1. 데이터의 양 늘리기
- 데이터의 양이 적을 경우 특정 패턴이나 노이즈까지 쉽게 암기하게 됨
-> 결과적으로 과적합 발생률 증가
- 데이터의 양이 적을 경우 데이터 증식 또는 증강(Augmentation)을 진행
-> 이미지를 돌리거나 노이즈를 추가, 일부분을 수정
-> 자연어에서는 데이터를 번역 후 재번역을 통해 새로운 데이터를 만들어냄
--> 역번역(Back Translation)

2. 모델의 복잡도 줄이기
- 과적합 현상이 포착되었을 때, 인공 신경망 모델의 은닉층이나 매개변수의 수를 줄여 복잡도를 줄이는 방법
-> 인공 신경망에서 모델에 있는 매개변수들의 수를 모델의 수용력(Capacity)이라고 함

3. 가중치 규제(Regularization) 적용
- 복잡한 모델이 간단한 모델보다 과적합될 가능성이 높음
- 간단한 모델 == 매개변수가 적은 모델
- 복잡한 모델을 좀더 간단하게 하는 방법 중 하나로는 가중치 규제가 있음
- L1 규제(L1 Regularization)
-> 가중치 w들의 절대값 합계를 비용 함수에 추가 
-> 기존의 비용 함수에 모든 가중치에 대해서 Ramda | w를 더 한 값을 비용 함수
-> L1을 사용하게 되면
--> 비용 함수가 최소가 되게 하는 가중치와 편향을 찾는 동시에
--> 가중치들의 절대값의 합 또한 최소가 되어야함
--> 가중치 W의 값들은 0또는 0에 가까이 작아져야함 --> 모델에 따라 안쓰이는 경우도 있음
-> 어떤 특성들이 모델에 영향을 주고 있는지를 정확히 판단할 때 유용

- L2 규제(L2 Regularization)
-> 모든 가중치 w들의 제곱합을 비용 함수에 추가
-> 1/2 x Ramda x w^2
-> L2를 사용하게 되면
--> 가중치들의 제곱을 최소화
--> L1의 값이 0이 되는 반면, L2는 0에 가까워지기만 함
-> 일반적으로 L2가 더 잘 작동, 가중치 감쇠(Weight Decay)라고 부름

- Ramda값이 크다면 모델이 훈련 데이터에 대해서 적합한 매개 변수를 찾는 것보다
- 규제를 위해 추가된 항들을 작게 유지하는 것을 우선한다는 뜻
- 두 식 모두 비용 함수를 최소화하기 위해서는 가중치 w들의 값이 작아저야한다는 특징

4. 드롭아웃(Drop Out)
- 학습 과정에서 신경망의 일부를 사용하지 않는 방법
- 드롭아웃의 비율이 0.5라면 학습 과정마다 랜덤으로 절반의 뉴런만 사용함
- 신경망 학습 시에만 사용, 예측시에는 사용하지 않음
- 학습 시에 인공 신경망이 특정 뉴런 또는 특정 조합에 너무 의존적이게 되는 것을 방지
- 매번 랜덤 선택으로 뉴런들을사용하지 않음 -> 서로 다른 신경망들을 앙상블하여 사용하는 것 같은 효과

