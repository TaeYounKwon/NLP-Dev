피드 포워드 신경망 언어 모델(Neural Network Language Model, NNLM)이란?

기존 N-Gram 언어 모델의 한계
- 저번에도 확인했던 희소 문제(Sparsity Problem)이 있었음
- 또한 단어 사이의 의미적 관계 혹은 유사성을 표현 할 수 없었음

피드포워드 신경망 언어 모델(NLM)이란?
- 언어 모델은 주어진 단어 시퀀스로부터 다음 단어를 예측
- 예시로는 what will the fat cat sit on이란 예문에서
- what will the fat cat이란 단어 시퀀스가 입력으로 주어지면 다음 단어 sit을 예측하는 방식으로 훈련됨
- what, will, the, fat, cat, sit, on이란 단어가 원 핫 백터로 바뀌게 되고, 
- 위의 각 단어별 원 핫 벡터를 입력받아 sit의 원-핫벡터를 예측하는 문제
- NNLM은 N-Gram처럼 앞의 모든 단어를 참고 하는 것이 아닌 정해진 개수의 단어만을 참고함

NNLM은
- 4개의 층으로 구성
- 원-핫 백터의 값을 받는 Input Layer

-  투사층 Projection Layer(Linear)
-> 다른 은닉층과는 달리 활성화 함수가 존재하지 않음
-> 투사층의 크기를 M으로 설정하면, 각 입력 단어들은 투사층에서 V x M 크기의 가중치 행렬과 곱해짐
-> 원-핫 백터과 W행렬의 곱은 사실상 W행렬의 i번째 행을 그대로 가지고 오는것과 같음
--> 그로 인해 룩업 테이블(look-up table)라고 불림
-> V차원을 가지는 원-핫 백터는 이보다 더 차원이 작은 M차원의 벡터로 맵핑됨
-> 이 벡터들은 초기에는 랜덤한 값을 가지지만, 학습 과정에서 값이 계속 변경되는데
-> 이 단어 벡터를 임베딩 벡터(embedding vector)라고 함
-> 결국 간 단어가 테이블 룩업을 통해 임베딩 백터로 변경
-> 투사층에서 모든 임베딩 벡터들의 값이 연결됨(concatenate됨)

- 은닉층 Hidden Layer(nonlinear)
- 투사층의 결과는 h의 크기를 가지는 은닉층으로 들어감
- 일반적인 피드 포워드 신경망에서 은닉층을 지난다는 것은
-> 은닉층의 입력은 가중치 곱해진 후 편향이 더해져 활성화 함수의 입력이 된다는 의미

- 출력층
- 출력층으로 향해지면서 V크기를 가지는 가중치와 곱해지고 편향이 더해짐
- 입력이였던 원-핫 벡터들과 동일한 차원으로 출력되게됨
- 출력층에서는 활성화 함수로 소프트맥수를 사용

- 이 과정을 통해 결과적으로 단어간의 유사도를 계산할 수 있음
- 단, N-Gram처럼 알고 싶은 단어 앞의 몇개의 단어만 확인함으로...
-> 문장 전체에서의 분위기를 못 알아차릴수도...
-> 해결방법? RNN!