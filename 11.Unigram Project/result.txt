LSTM Model Summary:
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 embedding (Embedding)       (None, None, 256)         262144

 lstm (LSTM)                 (None, None, 256)         525312

 lstm_1 (LSTM)               (None, None, 256)         525312

 lstm_2 (LSTM)               (None, None, 256)         525312

 time_distributed (TimeDistr  (None, None, 1024)       263168
 ibuted)

=================================================================
Total params: 2,101,248
Trainable params: 2,101,248
Non-trainable params: 0
_________________________________________________________________
Transformer Model Summary:
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to
==================================================================================================
 decoder_input (InputLayer)     [(None, None)]       0           []

 embedding_2 (Embedding)        (None, None, 256)    262144      ['decoder_input[0][0]']

 multi_head_attention (MultiHea  (None, None, 256)   1051904     ['embedding_2[0][0]',
 dAttention)                                                      'embedding_2[0][0]']

 layer_normalization (LayerNorm  (None, None, 256)   512         ['multi_head_attention[0][0]']
 alization)

 encoder_input (InputLayer)     [(None, None)]       0           []

 dropout (Dropout)              (None, None, 256)    0           ['layer_normalization[0][0]']

 embedding_1 (Embedding)        (None, None, 256)    262144      ['encoder_input[0][0]']

 multi_head_attention_1 (MultiH  (None, None, 256)   1051904     ['dropout[0][0]',
 eadAttention)                                                    'embedding_1[0][0]',
                                                                  'embedding_1[0][0]']

 layer_normalization_1 (LayerNo  (None, None, 256)   512         ['multi_head_attention_1[0][0]']
 rmalization)

 dropout_1 (Dropout)            (None, None, 256)    0           ['layer_normalization_1[0][0]']

 multi_head_attention_2 (MultiH  (None, None, 256)   1051904     ['dropout_1[0][0]',
 eadAttention)                                                    'dropout_1[0][0]']

 layer_normalization_2 (LayerNo  (None, None, 256)   512         ['multi_head_attention_2[0][0]']
 rmalization)

 dropout_2 (Dropout)            (None, None, 256)    0           ['layer_normalization_2[0][0]']

 multi_head_attention_3 (MultiH  (None, None, 256)   1051904     ['dropout_2[0][0]',
 eadAttention)                                                    'embedding_1[0][0]',
                                                                  'embedding_1[0][0]']

 layer_normalization_3 (LayerNo  (None, None, 256)   512         ['multi_head_attention_3[0][0]']
 rmalization)

 dropout_3 (Dropout)            (None, None, 256)    0           ['layer_normalization_3[0][0]']

 multi_head_attention_4 (MultiH  (None, None, 256)   1051904     ['dropout_3[0][0]',
 eadAttention)                                                    'dropout_3[0][0]']

 layer_normalization_4 (LayerNo  (None, None, 256)   512         ['multi_head_attention_4[0][0]']
 rmalization)

 dropout_4 (Dropout)            (None, None, 256)    0           ['layer_normalization_4[0][0]']

 multi_head_attention_5 (MultiH  (None, None, 256)   1051904     ['dropout_4[0][0]',
 eadAttention)                                                    'embedding_1[0][0]',
                                                                  'embedding_1[0][0]']

 layer_normalization_5 (LayerNo  (None, None, 256)   512         ['multi_head_attention_5[0][0]']
 rmalization)

 dropout_5 (Dropout)            (None, None, 256)    0           ['layer_normalization_5[0][0]']

 time_distributed_1 (TimeDistri  (None, None, 1024)  263168      ['dropout_5[0][0]']
 buted)

==================================================================================================
Total params: 7,101,952
Trainable params: 7,101,952
Non-trainable params: 0
__________________________________________________________________________________________________
sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : 
trainer_spec {
  input: ./data/ptb.test.txt
  input_format:
  model_prefix: ptb_unigram
  model_type: UNIGRAM
  vocab_size: 1024
  self_test_sample_size: 0
  character_coverage: 0.9995
  input_sentence_size: 0
  shuffle_input_sentence: 1
  seed_sentencepiece_size: 1000000
  shrinking_factor: 0.75
  max_sentence_length: 4192
  num_threads: 16
  num_sub_iterations: 2
  max_sentencepiece_length: 16
  split_by_unicode_script: 1
  split_by_number: 1
  split_by_whitespace: 1
  split_digits: 0
  pretokenization_delimiter:
  treat_whitespace_as_suffix: 0
  allow_whitespace_only_pieces: 0
  required_chars:
  byte_fallback: 0
  vocabulary_output_piece_score: 1
  train_extremely_large_corpus: 0
  seed_sentencepieces_file:
  hard_vocab_limit: 1
  use_all_vocab: 0
  unk_id: 0
  bos_id: 1
  eos_id: 2
  pad_id: -1
  unk_piece: <unk>
  bos_piece: <s>
  eos_piece: </s>
  pad_piece: <pad>
  unk_surface:  ??
  enable_differential_privacy: 0
  differential_privacy_noise_level: 0
  differential_privacy_clipping_threshold: 0
}
normalizer_spec {
  name: nmt_nfkc
  add_dummy_prefix: 1
  remove_extra_whitespaces: 1
  escape_whitespaces: 1
  normalization_rule_tsv:
}
denormalizer_spec {}
trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.
trainer_interface.cc(185) LOG(INFO) Loading corpus: ./data/ptb.test.txt
trainer_interface.cc(409) LOG(INFO) Loaded all 3761 sentences
trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>
trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>
trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>
trainer_interface.cc(430) LOG(INFO) Normalizing sentences...
trainer_interface.cc(539) LOG(INFO) all chars count=423247
trainer_interface.cc(550) LOG(INFO) Done: 99.9572% characters are covered.
trainer_interface.cc(560) LOG(INFO) Alphabet size=34
trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999572
trainer_interface.cc(592) LOG(INFO) Done! preprocessed 3761 sentences.
unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...
unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=219302
unigram_model_trainer.cc(312) LOG(INFO) Initialized 15931 seed sentencepieces
trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 3761
trainer_interface.cc(609) LOG(INFO) Done! 6039
unigram_model_trainer.cc(602) LOG(INFO) Using 6039 sentences for EM training
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5744 obj=11.2097 num_tokens=10616 num_tokens/piece=1.84819
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4691 obj=9.50062 num_tokens=10786 num_tokens/piece=2.2993
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3517 obj=9.49476 num_tokens=11923 num_tokens/piece=3.39011
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3514 obj=9.43543 num_tokens=11926 num_tokens/piece=3.39385
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2635 obj=9.60189 num_tokens=13845 num_tokens/piece=5.25427
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2635 obj=9.52994 num_tokens=13857 num_tokens/piece=5.25882
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1976 obj=9.84443 num_tokens=16081 num_tokens/piece=8.13816
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1976 obj=9.77273 num_tokens=16082 num_tokens/piece=8.13866
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1482 obj=10.178 num_tokens=18454 num_tokens/piece=12.4521
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1482 obj=10.0791 num_tokens=18452 num_tokens/piece=12.4507
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1126 obj=11.1287 num_tokens=20744 num_tokens/piece=18.4227
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1126 obj=11.0242 num_tokens=20751 num_tokens/piece=18.429
trainer_interface.cc(687) LOG(INFO) Saving model: ptb_unigram.model
trainer_interface.cc(699) LOG(INFO) Saving vocabs: ptb_unigram.vocab
Encoder inputs shape: (32, 124)
Decoder inputs shape: (32, 123)
Targets shape: (32, 123)
Epoch 1/10
2024-04-16 23:56:04.960317: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2024-04-16 23:56:05.108032: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8800
1315/1315 [==============================] - 88s 65ms/step - loss: 2.7000 - val_loss: 2.6194
Epoch 2/10
1315/1315 [==============================] - 88s 67ms/step - loss: 2.5222 - val_loss: 2.5047
Epoch 3/10
1315/1315 [==============================] - 90s 69ms/step - loss: 2.4484 - val_loss: 2.4634
Epoch 4/10
1315/1315 [==============================] - 90s 69ms/step - loss: 2.4088 - val_loss: 2.4468
Epoch 5/10
1315/1315 [==============================] - 90s 69ms/step - loss: 2.3836 - val_loss: 2.4302
Epoch 6/10
1315/1315 [==============================] - 91s 69ms/step - loss: 2.3645 - val_loss: 2.4196
Epoch 7/10
1315/1315 [==============================] - 90s 69ms/step - loss: 2.3491 - val_loss: 2.4138
Epoch 8/10
1315/1315 [==============================] - 90s 69ms/step - loss: 2.3368 - val_loss: 2.4044
Epoch 9/10
1315/1315 [==============================] - 90s 69ms/step - loss: 2.3256 - val_loss: 2.3979
Epoch 10/10
1315/1315 [==============================] - 90s 68ms/step - loss: 2.3169 - val_loss: 2.3946
118/118 [==============================] - 3s 25ms/step - loss: 2.3093
Perplexity: 10.067570686340332

Epoch 1/10
1315/1315 [==============================] - 47s 33ms/step - loss: 2.1058 - val_loss: 1.8939
Epoch 2/10
1315/1315 [==============================] - 43s 33ms/step - loss: 1.7522 - val_loss: 1.7364
Epoch 3/10
1315/1315 [==============================] - 43s 32ms/step - loss: 1.6291 - val_loss: 1.6381
Epoch 4/10
1315/1315 [==============================] - 42s 32ms/step - loss: 1.5378 - val_loss: 1.5589
Epoch 5/10
1315/1315 [==============================] - 43s 33ms/step - loss: 1.4599 - val_loss: 1.4933
Epoch 6/10
1315/1315 [==============================] - 43s 33ms/step - loss: 1.3934 - val_loss: 1.4411
Epoch 7/10
1315/1315 [==============================] - 46s 35ms/step - loss: 1.3385 - val_loss: 1.3989
Epoch 8/10
1315/1315 [==============================] - 44s 33ms/step - loss: 1.2939 - val_loss: 1.3663
Epoch 9/10
1315/1315 [==============================] - 43s 33ms/step - loss: 1.2577 - val_loss: 1.3398
Epoch 10/10
1315/1315 [==============================] - 42s 32ms/step - loss: 1.2273 - val_loss: 1.3190
118/118 [==============================] - 2s 13ms/step - loss: 1.2665
Perplexity: 3.548429012298584